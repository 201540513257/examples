{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from numpy import *\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts, n=None):\n",
    "    counter = Counter(''.join(texts))  # char level\n",
    "    char2index = {w: i for i, (w, c) in enumerate(counter.most_common(n), start=4)}\n",
    "    char2index['~'] = 0\n",
    "    char2index['^'] = 1\n",
    "    char2index['$'] = 2\n",
    "    char2index['#'] = 3\n",
    "    index2char = {i: w for w, i in char2index.items()}\n",
    "    return char2index, index2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = json.load(open('Time Dataset.json', 'rt', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = array(pairs)\n",
    "src_texts = data[:, 0]\n",
    "trg_texts = data[:, 1]\n",
    "src_c2ix, src_ix2c = build_vocab(src_texts)\n",
    "trg_c2ix, trg_ix2c = build_vocab(trg_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 6)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def indexes_from_text(text, char2index):\n",
    "    return [char2index[c] for c in text] + [2]\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [0 for _ in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "max_src_len = max(list(map(len, src_texts))) + 1\n",
    "max_trg_len = max(list(map(len, trg_texts))) + 1\n",
    "max_src_len, max_trg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(batch_size, pairs, src_c2ix, trg_c2ix):\n",
    "    input_seqs, target_seqs = [], []\n",
    "\n",
    "    for i in random.choice(len(pairs), batch_size):\n",
    "        input_seqs.append(indexes_from_text(pairs[i][0], src_c2ix))\n",
    "        target_seqs.append(indexes_from_text(pairs[i][1], trg_c2ix))\n",
    "\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    input_var = torch.LongTensor(input_padded).transpose(0, 1)  \n",
    "    # seq_len x batch_size\n",
    "    target_var = torch.LongTensor(target_padded).transpose(0, 1)\n",
    "    input_var = input_var.to(device)\n",
    "    target_var = target_var.to(device)\n",
    "\n",
    "    return input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value):\n",
    "    \"\"\"query[B, H], key[B, L, H], value[B, L, H]\"\"\"\n",
    "    query = query.unsqueeze(1).repeat(1, key.size(1), 1)\n",
    "    # (B, L, H)\n",
    "    score = torch.sum(query * key, -1)\n",
    "    attn = F.softmax(score, -1).unsqueeze(1)\n",
    "    # (B, 1, L)\n",
    "    outputs = torch.matmul(attn, value)\n",
    "    return outputs.squeeze(1), attn.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder_embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.decoder_embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.encoder = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.decoder = nn.GRUCell(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(embedding_dim + hidden_dim * 2, output_dim)\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        batch_size = inputs.size(1)\n",
    "        max_len = targets.size(0)\n",
    "        # (L, B)\n",
    "        embedded = self.encoder_embedding(inputs)\n",
    "        targets = self.decoder_embedding(targets)\n",
    "        # (L, B, E)\n",
    "        encoder_outputs, hidden = self.encoder(embedded)\n",
    "        # initialize \n",
    "        decoder_outputs = torch.zeros(max_len, batch_size, self.output_dim).to(device)\n",
    "        decoder_input = torch.zeros((batch_size, self.embedding_dim)).to(device)\n",
    "        hidden = hidden.squeeze(0) # (B, H)\n",
    "        for i in range(max_len):\n",
    "            hidden = self.decoder(decoder_input, hidden)\n",
    "            # (B, H)\n",
    "            context, _ = attention(hidden, encoder_outputs.transpose(0, 1), encoder_outputs.transpose(0, 1))\n",
    "            output = F.log_softmax(F.relu(self.fc(torch.cat((decoder_input, hidden, context), 1))), 1)\n",
    "            decoder_outputs[i] = output\n",
    "            decoder_input = targets[i]\n",
    "\n",
    "        return decoder_outputs\n",
    "    \n",
    "    def predict(self, inputs, max_trg_len):\n",
    "        batch_size = inputs.size(1)\n",
    "        max_src_len = inputs.size(0)\n",
    "        # (L, B)\n",
    "        embedded = self.encoder_embedding(inputs)\n",
    "        # (L, B, E)\n",
    "        encoder_outputs, hidden = self.encoder(embedded)\n",
    "        # initialize \n",
    "        decoder_outputs = torch.zeros(max_trg_len, batch_size, self.output_dim).to(device)\n",
    "        decoder_input = torch.zeros((batch_size, self.embedding_dim)).to(device)\n",
    "        attn_weights = torch.zeros((max_trg_len, batch_size, max_src_len)).to(device)\n",
    "        hidden = hidden.squeeze(0) # (B, H)\n",
    "        for i in range(max_trg_len):\n",
    "            hidden = self.decoder(decoder_input, hidden)\n",
    "            # (B, H)\n",
    "            context, attn_weight = attention(hidden, encoder_outputs.transpose(0, 1), encoder_outputs.transpose(0, 1))\n",
    "            output = F.log_softmax(F.relu(self.fc(torch.cat((decoder_input, hidden, context), 1))), 1)\n",
    "            decoder_outputs[i] = output\n",
    "            _, indices = torch.max(output, 1)\n",
    "            decoder_input = self.decoder_embedding(indices)\n",
    "            attn_weights[i] = attn_weight\n",
    "        return decoder_outputs, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 0.8925\n",
      "current loss: 0.5474\n",
      "current loss: 0.3475\n",
      "current loss: 0.2679\n",
      "current loss: 0.2080\n",
      "current loss: 0.1273\n",
      "current loss: 0.1008\n",
      "current loss: 0.0785\n",
      "current loss: 0.0659\n",
      "current loss: 0.0445\n",
      "current loss: 0.0497\n",
      "current loss: 0.0419\n",
      "current loss: 0.0277\n",
      "current loss: 0.0203\n",
      "current loss: 0.0287\n",
      "current loss: 0.0239\n",
      "current loss: 0.0093\n",
      "current loss: 0.0055\n",
      "current loss: 0.0109\n",
      "current loss: 0.0158\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "batch_size = 256\n",
    "clip = 5\n",
    "model = Seq2seq(len(src_c2ix)+1, len(trg_c2ix)+1, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.NLLLoss(ignore_index=0).to(device)\n",
    "\n",
    "model.train()\n",
    "for batch_id in range(1, 2001):\n",
    "    src_seqs, _, trg_seqs, _ = random_batch(batch_size, pairs, src_c2ix, trg_c2ix)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(src_seqs, trg_seqs)\n",
    "    loss = criterion(output.view(-1, output.shape[2]), trg_seqs.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_id % 100 == 0:\n",
    "        print('current loss: {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator())\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator())\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate(model, text, src_c2ix, trg_ix2c):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seq = torch.LongTensor(indexes_from_text(text, src_c2ix)).view(-1, 1).to(device)\n",
    "        outputs, attn_weights = model.predict(seq, max_trg_len)\n",
    "        outputs = outputs.squeeze(1).cpu().numpy()\n",
    "        output_words = [trg_ix2c[np.argmax(word_prob)] for word_prob in outputs]\n",
    "        print(''.join(output_words))\n",
    "        show_attention(list(text), output_words, attn_weights.squeeze(1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:54$\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAADxCAYAAACK/X/vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqTETrSQ1ZqKVpMb+HxkOLEY+iqUaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c6c1f2518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'seven hours and fifty four am'\n",
    "evaluate(model, text, src_c2ix, trg_ix2c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
