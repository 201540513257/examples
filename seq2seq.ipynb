{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from numpy import *\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts, n=None):\n",
    "    counter = Counter(''.join(texts))  # char level\n",
    "    char2index = {w: i for i, (w, c) in enumerate(counter.most_common(n), start=4)}\n",
    "    char2index['~'] = 0\n",
    "    char2index['^'] = 1\n",
    "    char2index['$'] = 2\n",
    "    char2index['#'] = 3\n",
    "    index2char = {i: w for w, i in char2index.items()}\n",
    "    return char2index, index2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = json.load(open('Time Dataset.json', 'rt', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = array(pairs)\n",
    "src_texts = data[:, 0]\n",
    "trg_texts = data[:, 1]\n",
    "src_c2ix, src_ix2c = build_vocab(src_texts)\n",
    "trg_c2ix, trg_ix2c = build_vocab(trg_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def indexes_from_text(text, char2index):\n",
    "    return [char2index[c] for c in text] + [2]\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [0 for _ in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "max_src_len = max(list(map(len, src_texts))) + 1\n",
    "max_trg_len = max(list(map(len, trg_texts)))\n",
    "max_src_len, max_trg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(batch_size, pairs, src_c2ix, trg_c2ix):\n",
    "    input_seqs, target_seqs = [], []\n",
    "\n",
    "    for i in random.choice(len(pairs), batch_size):\n",
    "        input_seqs.append(indexes_from_text(pairs[i][0], src_c2ix))\n",
    "        target_seqs.append(indexes_from_text(pairs[i][1], trg_c2ix))\n",
    "\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    input_var = torch.LongTensor(input_padded).transpose(0, 1)  \n",
    "    # seq_len x batch_size\n",
    "    target_var = torch.LongTensor(target_padded).transpose(0, 1)\n",
    "    input_var = input_var.to(device)\n",
    "    target_var = target_var.to(device)\n",
    "\n",
    "    return input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder_embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.decoder_embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.encoder = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.decoder = nn.GRUCell(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(embedding_dim + hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        batch_size = inputs.size(1)\n",
    "        max_len = targets.size(0)\n",
    "        # (L, B)\n",
    "        embedded = self.encoder_embedding(inputs)\n",
    "        targets = self.decoder_embedding(targets)\n",
    "        # (L, B, E)\n",
    "        _, hidden = self.encoder(embedded)\n",
    "        # initialize \n",
    "        decoder_outputs = torch.zeros(max_len, batch_size, self.output_dim).to(device)\n",
    "        decoder_input = torch.zeros((batch_size, self.embedding_dim)).to(device)\n",
    "        hidden = hidden.squeeze(0) # (B, H)\n",
    "        for i in range(max_len):\n",
    "            hidden = self.decoder(decoder_input, hidden)\n",
    "            # (B, H)\n",
    "            output = F.log_softmax(F.relu(self.fc(torch.cat((decoder_input, hidden), 1))), 1)\n",
    "            decoder_outputs[i] = output\n",
    "            decoder_input = targets[i]\n",
    "\n",
    "        return decoder_outputs\n",
    "    \n",
    "    def predict(self, inputs, max_len):\n",
    "        batch_size = inputs.size(1)\n",
    "        # (L, B)\n",
    "        embedded = self.encoder_embedding(inputs)\n",
    "        # (L, B, E)\n",
    "        _, hidden = self.encoder(embedded)\n",
    "        # initialize \n",
    "        decoder_outputs = torch.zeros(max_len, batch_size, self.output_dim).to(device)\n",
    "        decoder_input = torch.zeros((batch_size, self.embedding_dim)).to(device)\n",
    "        hidden = hidden.squeeze(0) # (B, H)\n",
    "        for i in range(max_len):\n",
    "            hidden = self.decoder(decoder_input, hidden)\n",
    "            # (B, H)\n",
    "            output = F.log_softmax(F.relu(self.fc(torch.cat((decoder_input, hidden), 1))), 1)\n",
    "            decoder_outputs[i] = output\n",
    "            _, indices = torch.max(output, 1)\n",
    "            decoder_input = self.decoder_embedding(indices)\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss: 1.4000\n",
      "current loss: 1.3474\n",
      "current loss: 1.2065\n",
      "current loss: 0.9809\n",
      "current loss: 0.8259\n",
      "current loss: 0.4454\n",
      "current loss: 0.2851\n",
      "current loss: 0.1642\n",
      "current loss: 0.1069\n",
      "current loss: 0.0735\n",
      "current loss: 0.0670\n",
      "current loss: 0.0494\n",
      "current loss: 0.0228\n",
      "current loss: 0.0339\n",
      "current loss: 0.0163\n",
      "current loss: 0.0119\n",
      "current loss: 0.0105\n",
      "current loss: 0.0107\n",
      "current loss: 0.0085\n",
      "current loss: 0.0127\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "batch_size = 256\n",
    "clip = 2\n",
    "model = Seq2seq(len(src_c2ix)+1, len(trg_c2ix)+1, embedding_dim, hidden_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.NLLLoss(ignore_index=0).to(device)\n",
    "\n",
    "model.train()\n",
    "for batch_id in range(1, 2001):\n",
    "    src_seqs, _, trg_seqs, _ = random_batch(batch_size, pairs, src_c2ix, trg_c2ix)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(src_seqs, trg_seqs)\n",
    "    loss = criterion(output.view(-1, output.shape[2]), trg_seqs.view(-1))\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_id % 100 == 0:\n",
    "        print('current loss: {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, text, src_c2ix, trg_ix2c):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seq = torch.LongTensor(indexes_from_text(text, src_c2ix)).view(-1, 1).to(device)\n",
    "        outputs = model.predict(seq, max_trg_len + 1)\n",
    "        outputs = outputs.squeeze(1).cpu().numpy()\n",
    "        output_words = [trg_ix2c[np.argmax(word_prob)] for word_prob in outputs]\n",
    "        print(''.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:39$\n"
     ]
    }
   ],
   "source": [
    "text = '21 min before two pm.'\n",
    "evaluate(model, text, src_c2ix, trg_ix2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
